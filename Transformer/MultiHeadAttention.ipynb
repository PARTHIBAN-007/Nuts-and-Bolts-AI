{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b8ce0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Attention:\n",
    "    def __init__(self,input,wq,wk,wv):\n",
    "        self.input = input\n",
    "        self.wq = wq\n",
    "        self.wk = wk\n",
    "        self.wv = wv\n",
    "\n",
    "    def qkv(self):\n",
    "        self.q = self.input@self.wq\n",
    "        self.k = self.input@self.wk\n",
    "        self.v = self.input@self.wv\n",
    "        return self.q,self.k,self.v\n",
    "    \n",
    "    def get_k_transpose(self):\n",
    "        self.k_transpose =  torch.transpose(self.k,dim0=0,dim1 =1)\n",
    "        return self.k_transpose\n",
    "\n",
    "    def get_attention_scores(self):\n",
    "        self.attention_scores = (self.q@self.k_transpose)/torch.sqrt(torch.tensor(2))\n",
    "        return self.attention_scores\n",
    "    \n",
    "    def get_attention_outputs(self):\n",
    "        self.attention_outputs = F.softmax(self.attention_scores)@self.v\n",
    "        return self.attention_outputs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14e6d018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 6]),\n",
       " torch.Size([6, 2]),\n",
       " torch.Size([6, 2]),\n",
       " torch.Size([6, 2]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.tensor([[1,1,0,0,0,0],\n",
    "                      [1,0,0,0,0,0],\n",
    "                      [0,0,1,1,0,0],\n",
    "                      [0,0,0,1,0,0],\n",
    "                      [0,0,0,0,1,1]],dtype=float)\n",
    "wq1 = torch.tensor([[10,0],\n",
    "                   [0,10],\n",
    "                   [0,0],\n",
    "                   [0,0],\n",
    "                   [0,0],\n",
    "                   [0,0]],dtype=float)\n",
    "\n",
    "wk1 = torch.tensor([[2,0],\n",
    "                   [0,2],\n",
    "                   [0,0],\n",
    "                   [0,0],\n",
    "                   [0,0],\n",
    "                   [0,0]],dtype=float)\n",
    "\n",
    "wv1 = torch.tensor([[1,0],\n",
    "                   [0,1],\n",
    "                   [0,0],\n",
    "                   [0,0],\n",
    "                   [0,0],\n",
    "                   [0,0]],dtype=float)\n",
    "\n",
    "input.shape,wq1.shape,wk1.shape,wv1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73a80274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head 1\n",
      "Q tensor([[10., 10.],\n",
      "        [10.,  0.],\n",
      "        [ 0.,  0.],\n",
      "        [ 0.,  0.],\n",
      "        [ 0.,  0.]], dtype=torch.float64)\n",
      "K tensor([[2., 2.],\n",
      "        [2., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]], dtype=torch.float64)\n",
      "V tensor([[1., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]], dtype=torch.float64)\n",
      "K Transpose tensor([[2., 2., 0., 0., 0.],\n",
      "        [2., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "Attention Scores tensor([[28.2843, 14.1421,  0.0000,  0.0000,  0.0000],\n",
      "        [14.1421, 14.1421,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]], dtype=torch.float64)\n",
      "Attention Output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.5000],\n",
      "        [0.4000, 0.2000],\n",
      "        [0.4000, 0.2000],\n",
      "        [0.4000, 0.2000]], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Parthiban\\AppData\\Local\\Temp\\ipykernel_17468\\3043070999.py:26: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  self.attention_outputs = F.softmax(self.attention_scores)@self.v\n"
     ]
    }
   ],
   "source": [
    "head1 = Attention(input,wq1,wk1,wv1)\n",
    "print(\"Head 1\")\n",
    "q1,k1,v1 = head1.qkv()\n",
    "print(\"Q\",q1)\n",
    "print(\"K\",k1)\n",
    "print(\"V\",v1)\n",
    "k1_transpose = head1.get_k_transpose()\n",
    "print(\"K Transpose\",k1_transpose)\n",
    "attention_scores_1 = head1.get_attention_scores()\n",
    "print(\"Attention Scores\",attention_scores_1)\n",
    "attention_output_1 = head1.get_attention_outputs()\n",
    "print(\"Attention Output\",attention_output_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b35bb38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 6]),\n",
       " torch.Size([6, 2]),\n",
       " torch.Size([6, 2]),\n",
       " torch.Size([6, 2]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wq2 = torch.tensor([[0,0],\n",
    "                   [0,0],\n",
    "                   [10,0],\n",
    "                   [0,10],\n",
    "                   [0,0],\n",
    "                   [0,0]],dtype=float)\n",
    "\n",
    "wk2 = torch.tensor([[0,0],\n",
    "                   [0,0],\n",
    "                   [2,0],\n",
    "                   [0,2],\n",
    "                   [0,0],\n",
    "                   [0,0]],dtype=float)\n",
    "\n",
    "wv2 = torch.tensor([[0,0],\n",
    "                   [0,0],\n",
    "                   [1,0],\n",
    "                   [0,1],\n",
    "                   [0,0],\n",
    "                   [0,0]],dtype=float)\n",
    "\n",
    "input.shape,wq2.shape,wk2.shape,wv2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86527a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head 2\n",
      "Q tensor([[ 0.,  0.],\n",
      "        [ 0.,  0.],\n",
      "        [10., 10.],\n",
      "        [ 0., 10.],\n",
      "        [ 0.,  0.]], dtype=torch.float64)\n",
      "K tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [2., 2.],\n",
      "        [0., 2.],\n",
      "        [0., 0.]], dtype=torch.float64)\n",
      "V tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [1., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 0.]], dtype=torch.float64)\n",
      "K Transpose tensor([[0., 0., 2., 0., 0.],\n",
      "        [0., 0., 2., 2., 0.]], dtype=torch.float64)\n",
      "Attention Scores tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000, 28.2843, 14.1421,  0.0000],\n",
      "        [ 0.0000,  0.0000, 14.1421, 14.1421,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]], dtype=torch.float64)\n",
      "Attention Output tensor([[0.2000, 0.4000],\n",
      "        [0.2000, 0.4000],\n",
      "        [1.0000, 1.0000],\n",
      "        [0.5000, 1.0000],\n",
      "        [0.2000, 0.4000]], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Parthiban\\AppData\\Local\\Temp\\ipykernel_17468\\3043070999.py:26: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  self.attention_outputs = F.softmax(self.attention_scores)@self.v\n"
     ]
    }
   ],
   "source": [
    "head2 = Attention(input,wq2,wk2,wv2)\n",
    "print(\"Head 2\")\n",
    "q2,k2,v2 = head2.qkv()\n",
    "print(\"Q\",q2)\n",
    "print(\"K\",k2)\n",
    "print(\"V\",v2)\n",
    "k2_transpose = head2.get_k_transpose()\n",
    "print(\"K Transpose\",k2_transpose)\n",
    "attention_scores_2 = head2.get_attention_scores()\n",
    "print(\"Attention Scores\",attention_scores_2)\n",
    "attention_output_2 = head2.get_attention_outputs()\n",
    "print(\"Attention Output\",attention_output_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2d98224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 6]),\n",
       " torch.Size([6, 2]),\n",
       " torch.Size([6, 2]),\n",
       " torch.Size([6, 2]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wq3 = torch.tensor([[0,0],\n",
    "                   [0,0],\n",
    "                   [0,0],\n",
    "                   [0,0],\n",
    "                   [10,0],\n",
    "                   [0,10]],dtype=float)\n",
    "\n",
    "wk3 = torch.tensor([[0,0],\n",
    "                   [0,0],\n",
    "                   [0,0],\n",
    "                   [0,0],\n",
    "                   [2,0],\n",
    "                   [0,2]],dtype=float)\n",
    "\n",
    "wv3 = torch.tensor([[0,0],\n",
    "                   [0,0],\n",
    "                   [0,0],\n",
    "                   [0,0],\n",
    "                   [1,0],\n",
    "                   [0,1]],dtype=float)\n",
    "\n",
    "input.shape,wq3.shape,wk3.shape,wv3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58b4ad59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head 3\n",
      "Q tensor([[ 0.,  0.],\n",
      "        [ 0.,  0.],\n",
      "        [ 0.,  0.],\n",
      "        [ 0.,  0.],\n",
      "        [10., 10.]], dtype=torch.float64)\n",
      "K tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [2., 2.]], dtype=torch.float64)\n",
      "V tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [1., 1.]], dtype=torch.float64)\n",
      "K Transpose tensor([[0., 0., 0., 0., 2.],\n",
      "        [0., 0., 0., 0., 2.]], dtype=torch.float64)\n",
      "Attention Scores tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000, 28.2843]], dtype=torch.float64)\n",
      "Attention Output tensor([[0.2000, 0.2000],\n",
      "        [0.2000, 0.2000],\n",
      "        [0.2000, 0.2000],\n",
      "        [0.2000, 0.2000],\n",
      "        [1.0000, 1.0000]], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Parthiban\\AppData\\Local\\Temp\\ipykernel_17468\\3043070999.py:26: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  self.attention_outputs = F.softmax(self.attention_scores)@self.v\n"
     ]
    }
   ],
   "source": [
    "head3 = Attention(input,wq3,wk3,wv3)\n",
    "print(\"Head 3\")\n",
    "q3,k3,v3 = head3.qkv()\n",
    "print(\"Q\",q3)\n",
    "print(\"K\",k3)\n",
    "print(\"V\",v3)\n",
    "k3_transpose = head3.get_k_transpose()\n",
    "print(\"K Transpose\",k3_transpose)\n",
    "attention_scores_3 = head3.get_attention_scores()\n",
    "print(\"Attention Scores\",attention_scores_3)\n",
    "attention_output_3 = head3.get_attention_outputs()\n",
    "print(\"Attention Output\",attention_output_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a963eb74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000, 0.2000, 0.4000, 0.2000, 0.2000],\n",
       "        [1.0000, 0.5000, 0.2000, 0.4000, 0.2000, 0.2000],\n",
       "        [0.4000, 0.2000, 1.0000, 1.0000, 0.2000, 0.2000],\n",
       "        [0.4000, 0.2000, 0.5000, 1.0000, 0.2000, 0.2000],\n",
       "        [0.4000, 0.2000, 0.2000, 0.4000, 1.0000, 1.0000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_outputs = torch.concat((attention_output_1,attention_output_2,attention_output_3),dim=1)\n",
    "concatenated_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b0b9a29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 6])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w0 = torch.tensor([[1,0,0,0,0,0],\n",
    "                      [0,1,0,0,0,0],\n",
    "                      [0,0,1,0,0,0],\n",
    "                      [0,0,0,1,0,0],\n",
    "                      [0,0,0,0,1,0],\n",
    "                      [0,0,0,0,0,1]],dtype=float)\n",
    "w0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7dbbe556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000, 0.2000, 0.4000, 0.2000, 0.2000],\n",
       "        [1.0000, 0.5000, 0.2000, 0.4000, 0.2000, 0.2000],\n",
       "        [0.4000, 0.2000, 1.0000, 1.0000, 0.2000, 0.2000],\n",
       "        [0.4000, 0.2000, 0.5000, 1.0000, 0.2000, 0.2000],\n",
       "        [0.4000, 0.2000, 0.2000, 0.4000, 1.0000, 1.0000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = concatenated_outputs@w0\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12042c60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
